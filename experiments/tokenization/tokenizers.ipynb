{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Some experiments on tokenization\n",
    "\n",
    "## Setup\n",
    "### Some Helper Functions\n",
    "\n",
    "I like to use relative paths to load stuff, these functions enable this in a jupyter notebook."
   ],
   "id": "b1768307df616cc7"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:38.211731Z",
     "start_time": "2025-08-13T14:07:38.209236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Optional, Iterator, Iterable\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def get_current_path():\n",
    "    current_notebook = os.environ.get(\"JPY_SESSION_NAME\")\n",
    "    if current_notebook is None:\n",
    "        raise EnvironmentError(\"JPY_SESSION_NAME is not set\")\n",
    "    return Path(current_notebook).parent\n",
    "\n",
    "def get_project_path():\n",
    "    current_path = get_current_path()\n",
    "    project_path = current_path.parent.parent\n",
    "    project_git = project_path / \".git\"\n",
    "    if not project_git.exists() or (not project_git.is_dir()):\n",
    "        raise ValueError(\"Project Base directory not found\")\n",
    "    return project_path\n"
   ],
   "id": "3e9b9a0c419e8730",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load Example Text\n",
    "\n",
    "The examples should be in the `data/examples/` directory. Use the python script there to download the files on first run."
   ],
   "id": "783f3b586372b669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:38.235626Z",
     "start_time": "2025-08-13T14:07:38.214716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_filename = get_project_path() / \"data\" / \"examples\" / \"the-verdict.txt\"\n",
    "example_text = example_filename.read_text(encoding=\"utf-8\")\n",
    "\n",
    "result_markdown = f\"\"\"\n",
    "#### Metadata\n",
    "Total number of character: {len(example_text)}\n",
    "\n",
    "#### First Characters\n",
    "```\n",
    "{example_text[:99]}\n",
    "```\n",
    "\"\"\"\n",
    "display(Markdown(result_markdown))"
   ],
   "id": "7989ac8b1a3271ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### Metadata\nTotal number of character: 20479\n\n#### First Characters\n```\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n```\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:38.266064Z",
     "start_time": "2025-08-13T14:07:38.264756Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7e62ade7fb243648",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A simple Tokenizer",
   "id": "70cdb257b6d64f20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:38.319882Z",
     "start_time": "2025-08-13T14:07:38.314347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "basic_split_regex_string = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "basic_split_regex = re.compile(basic_split_regex_string)\n",
    "\n",
    "def token_generator(text):\n",
    "    tokens = basic_split_regex.split(text)\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            yield token\n",
    "\n",
    "example_tokens = [token for token in token_generator(example_text)]\n",
    "\n",
    "example_token_set= sorted(set(example_tokens))\n",
    "example_vocabulary_decode = {token_id:token for token_id, token in enumerate(example_token_set)}\n",
    "example_vocabulary_encode = {token:token_id for token_id, token in enumerate(example_token_set)}\n",
    "#print(example_vocabulary)\n",
    "\n",
    "\n",
    "result_markdown = f\"\"\"\n",
    "#### Metadata\n",
    "* Total number of Tokens: {len(example_tokens)}\n",
    "* Size of Vocabulary: {len(example_token_set)}\n",
    "#### First Tokens\n",
    "{\"\\n\".join([(lambda token: \"\".join([\"* `\", token, \"` (\", str(example_vocabulary_encode.get(token, \"?\")), \")\"]))(token) for token in example_tokens[:11]])}\n",
    "\"\"\"\n",
    "display(Markdown(result_markdown))\n"
   ],
   "id": "784abaf5e531468c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### Metadata\n* Total number of Tokens: 4690\n* Size of Vocabulary: 1130\n#### First Tokens\n* `I` (53)\n* `HAD` (44)\n* `always` (149)\n* `thought` (1003)\n* `Jack` (57)\n* `Gisburn` (38)\n* `rather` (818)\n* `a` (115)\n* `cheap` (256)\n* `genius` (486)\n* `--` (6)\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:38.370870Z",
     "start_time": "2025-08-13T14:07:38.363295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, id_to_token_vocab: Mapping[int, str],\n",
    "                       *,\n",
    "                       id_to_reserved_tokens: Optional[Mapping[int, str]] = None,\n",
    "                       invalid_token_id: Optional[int] = None,\n",
    "                       begin_of_text_id: Optional[int] = None,\n",
    "                       end_of_text_id: Optional[int] = None,\n",
    "                       split_regex_string: str = r'([,.:;?_!\"()\\']|--|\\s)',\n",
    "                       reserved_tokens_regex_string: str = r'^<\\|[a-zA-Z0-9_]+\\|>$',\n",
    "                       clean_punct_regex_string: str=r'\\s+([,.?!\"()\\'])'):\n",
    "        self.split_regex = re.compile(split_regex_string)\n",
    "        self.reserved_tokens_regex = re.compile(reserved_tokens_regex_string)\n",
    "        self.clean_punct_regex = re.compile(clean_punct_regex_string)\n",
    "        if any([self.reserved_tokens_regex.match(token) for token in id_to_token_vocab.values()]):\n",
    "            raise ValueError(\"id_to_token_vocab cannot include reserved tokens according to {reserved_tokens_regex_string}\")\n",
    "        self.id_to_token = dict(id_to_token_vocab)\n",
    "        self.reserved_tokens = id_to_reserved_tokens\n",
    "        token_id_set = set(self.id_to_token.keys())\n",
    "        if id_to_reserved_tokens is not None:\n",
    "            for reserved_token in id_to_reserved_tokens.values():\n",
    "                if not self.reserved_tokens_regex.match(reserved_token):\n",
    "                    raise ValueError(f\"Reserved token '{reserved_token}' is not allowed according to {reserved_tokens_regex_string}\")\n",
    "            reserved_tokens_id_set = set(id_to_reserved_tokens.keys())\n",
    "            if not reserved_tokens_id_set.issubset(token_id_set):\n",
    "                raise ValueError(\"Reserved tokens IDs must not be included in vocabulary\")\n",
    "            if any([invalid_token_id, begin_of_text_id, end_of_text_id] is None):\n",
    "                raise ValueError(\"invalid_token_id, begin_of_text_id, end_of_text_id must be provided if reserved_tokens are given\")\n",
    "            if any([token_id not in token_id_set for token_id in [invalid_token_id, begin_of_text_id, end_of_text_id]]):\n",
    "                raise ValueError(\"invalid_token_id, begin_of_text_id, end_of_text_id must be included in id_to_reserved_tokens\")\n",
    "            self.invalid_token_id = invalid_token_id\n",
    "            self.begin_of_text_id = begin_of_text_id\n",
    "            self.end_of_text_id = end_of_text_id\n",
    "        else:\n",
    "            if invalid_token_id is not None:\n",
    "                raise ValueError(\"Invalid token id is specified without provided reserved tokens\")\n",
    "            reserved_token_id_start = 2**math.ceil(math.log2(max(token_id_set) + 1))\n",
    "            self.invalid_token_id = reserved_token_id_start\n",
    "            self.begin_of_text_id = reserved_token_id_start+1\n",
    "            self.end_of_text_id = reserved_token_id_start+2\n",
    "            self.reserved_tokens = {\n",
    "                self.invalid_token_id: \"<|unk|>\",\n",
    "                self.begin_of_text_id: \"<|bot|>\",\n",
    "                self.end_of_text_id: \"<|eot|>\",\n",
    "                }\n",
    "\n",
    "        self.id_to_token.update(self.reserved_tokens)\n",
    "        self.token_to_id = {token:token_id for token_id,token in self.id_to_token.items()}\n",
    "\n",
    "    def _token_generator(self, text) -> Iterator[str]:\n",
    "        tokens = self.split_regex.split(text)\n",
    "        for token in tokens:\n",
    "            token = token.strip()\n",
    "            if token:\n",
    "                yield token\n",
    "\n",
    "    def encoder(self, text) -> Iterator[int]:\n",
    "        tokens = self._token_generator(text)\n",
    "        for token in tokens:\n",
    "            token_id = example_vocabulary_encode.get(token)\n",
    "            if token_id is None:\n",
    "                if self.reserved_tokens_regex.match(token):\n",
    "                    raise ValueError(f\"Could not find reserved token '{token}'\")\n",
    "                token_id = self.invalid_token_id\n",
    "            yield token_id\n",
    "\n",
    "    def encode(self, text) -> list[int]:\n",
    "        return [token_id for token_id in self.encoder(text)]\n",
    "\n",
    "    def encoder_batch(self, texts: Iterable[str]) -> Iterator[int]:\n",
    "        for text in texts:\n",
    "            yield self.begin_of_text_id\n",
    "            for token_id in self.encoder(text):\n",
    "                yield token_id\n",
    "            yield self.end_of_text_id\n",
    "\n",
    "    def encode_batch(self, texts: Iterable[str]) -> list[int]:\n",
    "        return [token_id for token_id in self.encoder_batch(texts)]\n",
    "\n",
    "    def decoder(self, token_ids: Iterable[int]) -> Iterator[str]:\n",
    "        invalid_token = self.id_to_token[self.invalid_token_id]\n",
    "        for token_id in token_ids:\n",
    "            yield self.id_to_token.get(token_id, invalid_token)\n",
    "\n",
    "    def decode(self, token_ids: Iterable[int]) -> str:\n",
    "        decoded_text = \" \".join(self.decoder(token_ids))\n",
    "        cleaned_text = self.clean_punct_regex.sub(r\"\\1\", decoded_text)\n",
    "        return cleaned_text\n",
    "\n",
    "example_tokenizer = SimpleTokenizer(example_vocabulary_decode)\n",
    "\n",
    "test_texts = [\n",
    "    \"This is a text with GarantiertUnbekannt! And so on.\",\n",
    "    \"The next one!\",\n",
    "]\n",
    "encoded_test = example_tokenizer.encode_batch(test_texts)\n",
    "roundtrip_test = example_tokenizer.decode(encoded_test)\n",
    "\n",
    "result_markdown = f\"\"\"\n",
    "#### Metadata\n",
    "* Size of Vocabulary: {len(example_token_set)}\n",
    "* Invalid Token ID: {example_tokenizer.invalid_token_id}\n",
    "* Begin of Text: {example_tokenizer.begin_of_text_id}\n",
    "* End of Text ID: {example_tokenizer.end_of_text_id}\n",
    "#### Test String\n",
    "{test_texts}\n",
    "#### Encoded Tokens\n",
    "{encoded_test}\n",
    "#### Roundtrip\n",
    "{roundtrip_test}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(result_markdown))"
   ],
   "id": "76830879967aef24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### Metadata\n* Size of Vocabulary: 1130\n* Invalid Token ID: 2048\n* Begin of Text: 2049\n* End of Text ID: 2050\n#### Test String\n['This is a text with GarantiertUnbekannt! And so on.', 'The next one!']\n#### Encoded Tokens\n[2049, 97, 584, 115, 2048, 1108, 2048, 0, 14, 908, 727, 7, 2050, 2049, 93, 708, 729, 0, 2050]\n#### Roundtrip\n<|bot|> This is a <|unk|> with <|unk|>! And so on. <|eot|> <|bot|> The next one! <|eot|>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A BPE Tokenizer\n",
    "\n"
   ],
   "id": "bfb97906406942a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install tiktoken",
   "id": "deb848bbd3e7cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:41.634384Z",
     "start_time": "2025-08-13T14:07:39.065975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \"of someunknownPlace.\" )\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ],
   "id": "861b5ef413c2ff61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:07:41.684150Z",
     "start_time": "2025-08-13T14:07:41.682437Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Generating Training examples",
   "id": "31db4b6a1a50d6a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:08:42.628831Z",
     "start_time": "2025-08-13T14:08:42.623700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import islice, repeat, chain\n",
    "from collections import deque\n",
    "\n",
    "def encoding_generator(texts: Iterable[str], *, tokenizer=tiktoken.get_encoding(\"gpt2\")) -> list[int]:\n",
    "    texts_list = [text for text in texts]\n",
    "    encodings = tokenizer.encode_batch(texts_list, allowed_special={\"<|endoftext|>\"})\n",
    "    eot_id = tokenizer.encode_single_token(\"<|endoftext|>\")\n",
    "    collected_encodings = list()\n",
    "    for encoding in encodings:\n",
    "        if encoding:\n",
    "            if collected_encodings:\n",
    "                collected_encodings.append(eot_id)\n",
    "            collected_encodings.extend(encoding)\n",
    "    return collected_encodings\n",
    "\n",
    "def samples_generator(texts: Iterable[str], *, tokenizer=tiktoken.get_encoding(\"gpt2\"), context_size=50) -> Iterator[tuple[list[int], list[int]]]:\n",
    "    padding_id = tokenizer.encode_single_token(\"<|endoftext|>\")\n",
    "    padding_iterator = repeat(padding_id, context_size-1)\n",
    "    encodings = encoding_generator(texts, tokenizer=tokenizer)\n",
    "    encoding_iterator = chain(padding_iterator, iter(encodings))\n",
    "    window = deque(islice(encoding_iterator, context_size), maxlen=context_size)\n",
    "    for x in encoding_iterator:\n",
    "        source_window = list(window)\n",
    "        window.append(x)\n",
    "        target_window = list(window)\n",
    "        yield (source_window, target_window)\n",
    "\n",
    "samples = samples_generator(test_texts, context_size=5)\n",
    "\n",
    "for sample in samples:\n",
    "    print(f\"{sample[0]}: {sample[1]}\")\n"
   ],
   "id": "6cfc5561d6e6089",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256, 50256, 50256, 50256, 1212]: [50256, 50256, 50256, 1212, 318]\n",
      "[50256, 50256, 50256, 1212, 318]: [50256, 50256, 1212, 318, 257]\n",
      "[50256, 50256, 1212, 318, 257]: [50256, 1212, 318, 257, 2420]\n",
      "[50256, 1212, 318, 257, 2420]: [1212, 318, 257, 2420, 351]\n",
      "[1212, 318, 257, 2420, 351]: [318, 257, 2420, 351, 402]\n",
      "[318, 257, 2420, 351, 402]: [257, 2420, 351, 402, 4741]\n",
      "[257, 2420, 351, 402, 4741]: [2420, 351, 402, 4741, 72]\n",
      "[2420, 351, 402, 4741, 72]: [351, 402, 4741, 72, 861]\n",
      "[351, 402, 4741, 72, 861]: [402, 4741, 72, 861, 3118]\n",
      "[402, 4741, 72, 861, 3118]: [4741, 72, 861, 3118, 47083]\n",
      "[4741, 72, 861, 3118, 47083]: [72, 861, 3118, 47083, 272]\n",
      "[72, 861, 3118, 47083, 272]: [861, 3118, 47083, 272, 429]\n",
      "[861, 3118, 47083, 272, 429]: [3118, 47083, 272, 429, 0]\n",
      "[3118, 47083, 272, 429, 0]: [47083, 272, 429, 0, 843]\n",
      "[47083, 272, 429, 0, 843]: [272, 429, 0, 843, 523]\n",
      "[272, 429, 0, 843, 523]: [429, 0, 843, 523, 319]\n",
      "[429, 0, 843, 523, 319]: [0, 843, 523, 319, 13]\n",
      "[0, 843, 523, 319, 13]: [843, 523, 319, 13, 50256]\n",
      "[843, 523, 319, 13, 50256]: [523, 319, 13, 50256, 464]\n",
      "[523, 319, 13, 50256, 464]: [319, 13, 50256, 464, 1306]\n",
      "[319, 13, 50256, 464, 1306]: [13, 50256, 464, 1306, 530]\n",
      "[13, 50256, 464, 1306, 530]: [50256, 464, 1306, 530, 0]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate pytorch Dataset",
   "id": "4fe22b52ac4e99d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:21:07.045271Z",
     "start_time": "2025-08-13T14:21:06.986453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, *, tokenizer=tiktoken.get_encoding(\"gpt2\"), context_size=50, stride=1):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        samples = samples_generator([txt], tokenizer=tokenizer, context_size=context_size)\n",
    "        sliced_samples = islice(samples, None, None, stride)\n",
    "        for sample in samples:\n",
    "            self.input_ids.append(torch.tensor(sample[0]))\n",
    "            self.target_ids.append(torch.tensor(sample[1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt, *, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer=tokenizer, context_size=max_length, stride=stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers )\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader(example_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ],
   "id": "d1d45ee940c5b3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[50256, 50256, 50256,    40]]), tensor([[50256, 50256,    40,   367]])]\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
