{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Some experiments on tokenization\n",
    "\n",
    "## Setup\n",
    "### Some Helper Functions\n",
    "\n",
    "I like to use relative paths to load stuff, these functions enable this in a jupyter notebook."
   ],
   "id": "b1768307df616cc7"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T12:25:41.213449Z",
     "start_time": "2025-08-13T12:25:41.211107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Optional, Iterator, Iterable\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def get_current_path():\n",
    "    current_notebook = os.environ.get(\"JPY_SESSION_NAME\")\n",
    "    if current_notebook is None:\n",
    "        raise EnvironmentError(\"JPY_SESSION_NAME is not set\")\n",
    "    return Path(current_notebook).parent\n",
    "\n",
    "def get_project_path():\n",
    "    current_path = get_current_path()\n",
    "    project_path = current_path.parent.parent\n",
    "    project_git = project_path / \".git\"\n",
    "    if not project_git.exists() or (not project_git.is_dir()):\n",
    "        raise ValueError(\"Project Base directory not found\")\n",
    "    return project_path\n"
   ],
   "id": "3e9b9a0c419e8730",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load Example Text\n",
    "\n",
    "The examples should be in the `data/examples/` directory. Use the python script there to download the files on first run."
   ],
   "id": "783f3b586372b669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:05:17.371894Z",
     "start_time": "2025-08-13T11:05:17.368882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_filename = get_project_path() / \"data\" / \"examples\" / \"the-verdict.txt\"\n",
    "example_text = example_filename.read_text(encoding=\"utf-8\")\n",
    "\n",
    "result_markdown = f\"\"\"\n",
    "#### Metadata\n",
    "Total number of character: {len(example_text)}\n",
    "\n",
    "#### First Characters\n",
    "```\n",
    "{example_text[:99]}\n",
    "```\n",
    "\"\"\"\n",
    "display(Markdown(result_markdown))"
   ],
   "id": "7989ac8b1a3271ad",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### Metadata\nTotal number of character: 20479\n\n#### First Characters\n```\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n```\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7e62ade7fb243648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A simple Tokenizer",
   "id": "70cdb257b6d64f20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:29:45.348996Z",
     "start_time": "2025-08-13T11:29:45.343804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "basic_split_regex_string = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "basic_split_regex = re.compile(basic_split_regex_string)\n",
    "\n",
    "def token_generator(text):\n",
    "    tokens = basic_split_regex.split(text)\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            yield token\n",
    "\n",
    "example_tokens = [token for token in token_generator(example_text)]\n",
    "\n",
    "example_token_set= sorted(set(example_tokens))\n",
    "example_vocabulary_decode = {token_id:token for token_id, token in enumerate(example_token_set)}\n",
    "example_vocabulary_encode = {token:token_id for token_id, token in enumerate(example_token_set)}\n",
    "#print(example_vocabulary)\n",
    "\n",
    "\n",
    "result_markdown = f\"\"\"\n",
    "#### Metadata\n",
    "* Total number of Tokens: {len(example_tokens)}\n",
    "* Size of Vocabulary: {len(example_token_set)}\n",
    "#### First Tokens\n",
    "{\"\\n\".join([(lambda token: \"\".join([\"* `\", token, \"` (\", str(example_vocabulary_encode.get(token, \"?\")), \")\"]))(token) for token in example_tokens[:11]])}\n",
    "\"\"\"\n",
    "display(Markdown(result_markdown))\n"
   ],
   "id": "784abaf5e531468c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### Metadata\n* Total number of Tokens: 4690\n* Size of Vocabulary: 1130\n#### First Tokens\n* `I` (53)\n* `HAD` (44)\n* `always` (149)\n* `thought` (1003)\n* `Jack` (57)\n* `Gisburn` (38)\n* `rather` (818)\n* `a` (115)\n* `cheap` (256)\n* `genius` (486)\n* `--` (6)\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T12:59:56.012333Z",
     "start_time": "2025-08-13T12:59:56.004719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, id_to_token_vocab: Mapping[int, str],\n",
    "                       *,\n",
    "                       id_to_reserved_tokens: Optional[Mapping[int, str]] = None,\n",
    "                       invalid_token_id: Optional[int] = None,\n",
    "                       begin_of_text_id: Optional[int] = None,\n",
    "                       end_of_text_id: Optional[int] = None,\n",
    "                       split_regex_string: str = r'([,.:;?_!\"()\\']|--|\\s)',\n",
    "                       reserved_tokens_regex_string: str = r'^<\\|[a-zA-Z0-9_]+\\|>$',\n",
    "                       clean_punct_regex_string: str=r'\\s+([,.?!\"()\\'])'):\n",
    "        self.split_regex = re.compile(split_regex_string)\n",
    "        self.reserved_tokens_regex = re.compile(reserved_tokens_regex_string)\n",
    "        self.clean_punct_regex = re.compile(clean_punct_regex_string)\n",
    "        if any([self.reserved_tokens_regex.match(token) for token in id_to_token_vocab.values()]):\n",
    "            raise ValueError(\"id_to_token_vocab cannot include reserved tokens according to {reserved_tokens_regex_string}\")\n",
    "        self.id_to_token = dict(id_to_token_vocab)\n",
    "        self.reserved_tokens = id_to_reserved_tokens\n",
    "        token_id_set = set(self.id_to_token.keys())\n",
    "        if id_to_reserved_tokens is not None:\n",
    "            for reserved_token in id_to_reserved_tokens.values():\n",
    "                if not self.reserved_tokens_regex.match(reserved_token):\n",
    "                    raise ValueError(f\"Reserved token '{reserved_token}' is not allowed according to {reserved_tokens_regex_string}\")\n",
    "            reserved_tokens_id_set = set(id_to_reserved_tokens.keys())\n",
    "            if not reserved_tokens_id_set.issubset(token_id_set):\n",
    "                raise ValueError(\"Reserved tokens IDs must not be included in vocabulary\")\n",
    "            if any([invalid_token_id, begin_of_text_id, end_of_text_id] is None):\n",
    "                raise ValueError(\"invalid_token_id, begin_of_text_id, end_of_text_id must be provided if reserved_tokens are given\")\n",
    "            if any([token_id not in token_id_set for token_id in [invalid_token_id, begin_of_text_id, end_of_text_id]]):\n",
    "                raise ValueError(\"invalid_token_id, begin_of_text_id, end_of_text_id must be included in id_to_reserved_tokens\")\n",
    "            self.invalid_token_id = invalid_token_id\n",
    "            self.begin_of_text_id = begin_of_text_id\n",
    "            self.end_of_text_id = end_of_text_id\n",
    "        else:\n",
    "            if invalid_token_id is not None:\n",
    "                raise ValueError(\"Invalid token id is specified without provided reserved tokens\")\n",
    "            reserved_token_id_start = 2**math.ceil(math.log2(max(token_id_set) + 1))\n",
    "            self.invalid_token_id = reserved_token_id_start\n",
    "            self.begin_of_text_id = reserved_token_id_start+1\n",
    "            self.end_of_text_id = reserved_token_id_start+2\n",
    "            self.reserved_tokens = {\n",
    "                self.invalid_token_id: \"<|unk|>\",\n",
    "                self.begin_of_text_id: \"<|bot|>\",\n",
    "                self.end_of_text_id: \"<|eot|>\",\n",
    "                }\n",
    "\n",
    "        self.id_to_token.update(self.reserved_tokens)\n",
    "        self.token_to_id = {token:token_id for token_id,token in self.id_to_token.items()}\n",
    "\n",
    "    def _token_generator(self, text) -> Iterator[str]:\n",
    "        tokens = self.split_regex.split(text)\n",
    "        for token in tokens:\n",
    "            token = token.strip()\n",
    "            if token:\n",
    "                yield token\n",
    "\n",
    "    def encoder(self, text) -> Iterator[int]:\n",
    "        tokens = self._token_generator(text)\n",
    "        for token in tokens:\n",
    "            token_id = example_vocabulary_encode.get(token)\n",
    "            if token_id is None:\n",
    "                if self.reserved_tokens_regex.match(token):\n",
    "                    raise ValueError(f\"Could not find reserved token '{token}'\")\n",
    "                token_id = self.invalid_token_id\n",
    "            yield token_id\n",
    "\n",
    "    def encode(self, text) -> list[int]:\n",
    "        return [token_id for token_id in self.encoder(text)]\n",
    "\n",
    "    def encoder_batch(self, texts: Iterable[str]) -> Iterator[int]:\n",
    "        for text in texts:\n",
    "            yield self.begin_of_text_id\n",
    "            for token_id in self.encoder(text):\n",
    "                yield token_id\n",
    "            yield self.end_of_text_id\n",
    "\n",
    "    def encode_batch(self, texts: Iterable[str]) -> list[int]:\n",
    "        return [token_id for token_id in self.encoder_batch(texts)]\n",
    "\n",
    "    def decoder(self, token_ids: Iterable[int]) -> Iterator[str]:\n",
    "        invalid_token = self.id_to_token[self.invalid_token_id]\n",
    "        for token_id in token_ids:\n",
    "            yield self.id_to_token.get(token_id, invalid_token)\n",
    "\n",
    "    def decode(self, token_ids: Iterable[int]) -> str:\n",
    "        decoded_text = \" \".join(self.decoder(token_ids))\n",
    "        cleaned_text = self.clean_punct_regex.sub(r\"\\1\", decoded_text)\n",
    "        return cleaned_text\n",
    "\n",
    "example_tokenizer = SimpleTokenizer(example_vocabulary_decode)\n",
    "\n",
    "test_texts = [\n",
    "    \"This is a text with GarantiertUnbekannt! And so on.\",\n",
    "    \"The next one!\",\n",
    "]\n",
    "encoded_test = example_tokenizer.encode_batch(test_texts)\n",
    "roundtrip_test = example_tokenizer.decode(encoded_test)\n",
    "\n",
    "result_markdown = f\"\"\"\n",
    "#### Metadata\n",
    "* Size of Vocabulary: {len(example_token_set)}\n",
    "* Invalid Token ID: {example_tokenizer.invalid_token_id}\n",
    "* Begin of Text: {example_tokenizer.begin_of_text_id}\n",
    "* End of Text ID: {example_tokenizer.end_of_text_id}\n",
    "#### Test String\n",
    "{test_texts}\n",
    "#### Encoded Tokens\n",
    "{encoded_test}\n",
    "#### Roundtrip\n",
    "{roundtrip_test}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(result_markdown))"
   ],
   "id": "76830879967aef24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n#### Metadata\n* Size of Vocabulary: 1130\n* Invalid Token ID: 2048\n* Begin of Text: 2049\n* End of Text ID: 2050\n#### Test String\n['This is a text with GarantiertUnbekannt! And so on.', 'The next one!']\n#### Encoded Tokens\n[2049, 97, 584, 115, 2048, 1108, 2048, 0, 14, 908, 727, 7, 2050, 2049, 93, 708, 729, 0, 2050]\n#### Roundtrip\n<|bot|> This is a <|unk|> with <|unk|>! And so on. <|eot|> <|bot|> The next one! <|eot|>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A BPE Tokenizer\n",
    "\n"
   ],
   "id": "bfb97906406942a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install tiktoken",
   "id": "deb848bbd3e7cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:08:28.673585Z",
     "start_time": "2025-08-13T13:08:28.670302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \"of someunknownPlace.\" )\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ],
   "id": "861b5ef413c2ff61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
